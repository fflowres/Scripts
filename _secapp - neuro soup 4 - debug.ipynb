{
 "metadata": {
  "name": "",
  "signature": "sha256:c1540fe949b09c31e17c5391db76082f2173a1406ea88ddbaff185098e6eb763"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "data normalizer ref:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#For arbitrary date, daily point datastructures, averages points that are non existing\n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "import datetime\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "initDO = SecuFrame('')\n",
      "\n",
      "timeObjs = initDO.iffo().orderedmap().processFrameList\n",
      "keyOfInterest = 'dependant_question'\n",
      "normConst = datetime.timedelta(minutes = 20 )\n",
      "\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalized_dtokey_dict\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalizedX\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalizedY"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "FULL TEST CLASS"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Algo Parser Selector class "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "            \n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "import datetime\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "\n",
      "\n",
      "class NeuroTrainer(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        #pass mynet object\n",
      "        NeuroTrainer.mynet = None\n",
      "\n",
      "        \n",
      "        \n",
      "    def aquiredata(self):\n",
      "        self.initDO = SecuFrame('')\n",
      "        self.fromDB = self.initDO.iffo().orderedmap().processFrameList\n",
      "                \n",
      "        return self\n",
      "    \n",
      "    def methodBasicTrain(self):\n",
      "        for selectionPointVariable in NeuroBasicConditions.pointSelectionConfig.keys(): # iterate through point sets\n",
      "            \n",
      "            #Normalized points dataset(object)\n",
      "            #print NeuroBasicConditions.pointSelectionConfig[selectionPointVariable][\"NormConstant\"]\n",
      "            #print \"--------------------\"\n",
      "            #print NeuroBasicConditions.pointSelectionConfig[selectionPointVariable]\n",
      "            #print \"--------------------\"\n",
      "            #print NeuroBasicConditions.pointSelectionConfig\n",
      "            \n",
      "            normalizedSelectionPointData = DataNormalization(self.fromDB,NeuroBasicConditions.pointSelectionConfig[selectionPointVariable][\"pointSelectSet\"],NeuroBasicConditions.pointSelectionConfig[selectionPointVariable][\"NormConstant\"])\n",
      "            normalizedSelectionPointDataID = NeuroBasicConditions.pointSelectionConfig[selectionPointVariable][\"ID\"]\n",
      "            #computed selection points\n",
      "            \n",
      "            NeuroPoints.__init__(self,normalizedSelectionPointDataID,normalizedSelectionPointData.normalizedX,normalizedSelectionPointData.normalizedY)\n",
      "            computedSelectionPoints = NeuroPoints.XpointsofInterest\n",
      "            \n",
      "            #print \"computedSelectionPoints \" + str(computedSelectionPoints)\n",
      "            \n",
      "            for analyzableDatasetVariable in NeuroBasicConditions.datasetConfig.keys(): # iterate through datasets\n",
      "                \n",
      "                #Normalize dataset (object)\n",
      "                normalizedAnalyzableDataset = DataNormalization(self.fromDB,analyzableDatasetVariable,NeuroBasicConditions.datasetConfig[analyzableDatasetVariable][\"NormConstant\"])\n",
      "                normalizedAnalyzableDatasetID = NeuroBasicConditions.datasetConfig[analyzableDatasetVariable][\"ID\"]\n",
      "                #compute each condition and train neural network\n",
      "                \n",
      "                \n",
      "                NeuroCondSearch.__init__(self, normalizedAnalyzableDataset.normalizedX, normalizedAnalyzableDataset.normalizedY)\n",
      "                \n",
      "                for computeConditionID in NeuroBasicConditions.DatasetConditions[normalizedAnalyzableDatasetID]: # returns ids of conditions attached to dataset\n",
      "                    \n",
      "                    #initialize conditional search, use id\n",
      "                    NeuroCondSearch.setID(self, computeConditionID)\n",
      "                    \n",
      "                    #print \"computeConditionID \" + str(computeConditionID)\n",
      "                    for xpoint in computedSelectionPoints: # iterate through each point\n",
      "                        bool_result = NeuroCondSearch.runop(self,xpoint)\n",
      "                        #Train neuralNet\n",
      "                        \n",
      "                        #Level 1\n",
      "                        #mynet.trainquery([words],[urls],url clicked)\n",
      "                        #mynet.trainquery([conditions],[point of interest method + set], point of interest method ok)\n",
      "                        '''\n",
      "                        TEST TYPE:\n",
      "                        query conditions, output selection point id\n",
      "                        \n",
      "                        invert map:\n",
      "                        \n",
      "                        query selection point sets, output conditions\n",
      "                        '''\n",
      "                        #print normalizedSelectionPointDataID\n",
      "                        #print \"no Training on Xpoint: \" + str(xpoint)\n",
      "                        if (bool_result):\n",
      "                            #print \"= TRAIN WAS OK; Xpoint:  \" + str(xpoint)  + \"computeConditionID: \" + str(computeConditionID) + \" normalizedAnalyzableDatasetID: \" + str(normalizedAnalyzableDatasetID) + \" normalizedSelectionPointDataID: \" + str(normalizedSelectionPointDataID)\n",
      "                            NeuroTrainer.mynet.trainquery(NeuroBasicConditions.allPointSelectionIds,NeuroBasicConditions.allConditionIds, computeConditionID)\n",
      "\n",
      "                            \n",
      "from SecApp.DBQuestionClass import SecuQ\n",
      "class NeuroBasicConditions(object):\n",
      "    def __init__(self,key,depVarsLst,indepVarsLst):\n",
      "        self.depVarsLst = depVarsLst\n",
      "        self.indepVarsLst = indepVarsLst\n",
      "        self.key = key\n",
      "        self.initQ = SecuQ(self.key)\n",
      "        '''\n",
      "        available parameters used to construct conditions\n",
      "        '''\n",
      "        #analytics conditions\n",
      "        self.baseconditions = [\"increase\"]\n",
      "        self.pointofinteresttype = [\"localmax\"]\n",
      "        #more to be added\n",
      "        #baseconditions = {\"increase\":1,\"decrease\":2,\"MA14_increase\":3,\"MA14_decrease\":4}\n",
      "        self.phasedeltas = [2,3,4,5,6,7,14,21] #how much data in each segment \n",
      "        self.phaseshifts = [1,2,3,4,5,6,7,14,21] # how much data is shifted from phase ( x coord[date])\n",
      "    \n",
      "        '''\n",
      "        constructed conditions (with different maps for easy access)\n",
      "        '''\n",
      "\n",
      "    \n",
      "        NeuroBasicConditions.conditionsConfig = {} #analysis algo conditions\n",
      "        NeuroBasicConditions.conditionLookupId = {}\n",
      "        NeuroBasicConditions.idLookupCondtion = {}\n",
      "        NeuroBasicConditions.allConditionIds = []\n",
      "        NeuroBasicConditions.DatasetConditions = {}\n",
      "        \n",
      "    \n",
      "    \n",
      "        NeuroBasicConditions.pointSelectionConfig = {} #dependant IDs point selection algo\n",
      "        NeuroBasicConditions.pointSelectionLookupId = {}\n",
      "        NeuroBasicConditions.idLookupPointSelection = {}\n",
      "        NeuroBasicConditions.allPointSelectionIds = []\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "        NeuroBasicConditions.datasetConfig = {} #independantID datasets available\n",
      "        NeuroBasicConditions.datasetLookupId = {}\n",
      "        NeuroBasicConditions.idLookupDataset = {}\n",
      "        NeuroBasicConditions.allDatasetIds = []\n",
      "        \n",
      "    \n",
      "        '''\n",
      "        this holds starting id numbers for configurtation data \n",
      "        '''\n",
      "        ###\n",
      "        self.depIDcount = 300\n",
      "        \n",
      "        self.indepIDcount = 500\n",
      "        \n",
      "        self.condIDcount = 700\n",
      "    \n",
      "        self.buildselectionpointIDs()\n",
      "        self.buildindependantIDs()\n",
      "        self.buildconditions()\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "    def getNormConstant(self,varkey):  # for variables only\n",
      "        if (varkey in self.initQ.multipoint.keys()) & (varkey in self.initQ.aggregate.keys()):\n",
      "            #self.varTypes_v2t_dict[varkey] = 'mp_aggre' # multipoint with aggregate data in dailykey, \"keyname\"_aggregate:value\n",
      "            \n",
      "            #self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'mp_aggre'\n",
      "            raise Exception(\"NO MULTI-AGREGATE VARIABLES (IN ALPHA)\")\n",
      "            return False #breakloop\n",
      "            \n",
      "        if (varkey in self.initQ.aggregate.keys()):\n",
      "            return datetime.timedelta(days = 1 )\n",
      "        \n",
      "        if (varkey in self.initQ.multipoint.keys()):\n",
      "            return datetime.timedelta(minutes = 30 )\n",
      "            \n",
      "        if (varkey in self.initQ.active.keys()):\n",
      "            return datetime.timedelta(days = 1 )\n",
      "  \n",
      "    \n",
      "    \n",
      "    def buildselectionpointIDs(self): #builddependantIDs\n",
      "        for pointselectionalgo in self.pointofinteresttype:\n",
      "            for var in self.depVarsLst:\n",
      "                \n",
      "                builtPointSelectionTmp = str(var) + \"-\" + str(pointselectionalgo)+ \"-\" + str(self.depIDcount)\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp] = {}\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"pointSelectSet\"] = var\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"selection_algo\"] = pointselectionalgo\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"ID\"] = self.depIDcount\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"NormConstant\"] = self.getNormConstant(var)\n",
      "              \n",
      "                NeuroBasicConditions.pointSelectionLookupId[builtPointSelectionTmp] = self.depIDcount\n",
      "            \n",
      "                NeuroBasicConditions.idLookupPointSelection[self.depIDcount] = builtPointSelectionTmp\n",
      "            \n",
      "                NeuroBasicConditions.allPointSelectionIds.append(self.depIDcount)\n",
      "            \n",
      "                self.depIDcount+= 1\n",
      "        return self\n",
      "            \n",
      "                \n",
      "            \n",
      "            \n",
      "    def buildindependantIDs(self): #build analyzable (comparable dataset) ids\n",
      "        for var in self.indepVarsLst:\n",
      "            NeuroBasicConditions.datasetConfig[var] = {}\n",
      "            NeuroBasicConditions.datasetConfig[var][\"ID\"] = self.indepIDcount\n",
      "            NeuroBasicConditions.datasetConfig[var][\"NormConstant\"] = self.getNormConstant(var)\n",
      "            \n",
      "            NeuroBasicConditions.datasetLookupId[var] = self.indepIDcount\n",
      "            \n",
      "            NeuroBasicConditions.idLookupDataset[self.indepIDcount] = var\n",
      "            \n",
      "            NeuroBasicConditions.allDatasetIds.append(self.indepIDcount)\n",
      "            \n",
      "            \n",
      "            self.indepIDcount+=1\n",
      "        return self\n",
      "\n",
      "    \n",
      "    def buildconditions(self): # add [haseshifts]\n",
      "        for var in NeuroBasicConditions.datasetConfig.keys():\n",
      "            currVarID = NeuroBasicConditions.datasetConfig[var][\"ID\"]\n",
      "            NeuroBasicConditions.DatasetConditions[currVarID] = []\n",
      "            \n",
      "            for condition in self.baseconditions:\n",
      "                for delta in self.phasedeltas:\n",
      "                    for shift in self.phaseshifts:\n",
      "                        \n",
      "                        \n",
      "                    \n",
      "                        builtconditiontmp = str(condition) + \"-\" + str(currVarID)+ \"-\" + str(delta)+ \"-\" + str(shift)\n",
      "                    \n",
      "                        \n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp] = {}\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp][\"analyze\"] = currVarID\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['condition'] = condition\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['delta'] = delta\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['shift'] = shift\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['ID'] = self.condIDcount\n",
      "                        \n",
      "                        \n",
      "                        NeuroBasicConditions.DatasetConditions[currVarID].append(self.condIDcount)\n",
      "                        \n",
      "                        NeuroBasicConditions.idLookupCondtion[self.condIDcount] = builtconditiontmp\n",
      "                    \n",
      "                        NeuroBasicConditions.conditionLookupId[builtconditiontmp] = self.condIDcount\n",
      "                    \n",
      "                        NeuroBasicConditions.allConditionIds.append(self.condIDcount)\n",
      "                    \n",
      "                        self.condIDcount+=1\n",
      "        return self\n",
      "    \n",
      "\n",
      "class NeuroPoints(object):\n",
      "    def __init__(self,ID,datasetX,datasetY):      \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        self.datasetID = ID\n",
      "        \n",
      "        ## must be initialized before hand\n",
      "        self.PointTypeName = NeuroBasicConditions.idLookupPointSelection[self.datasetID]\n",
      "        self.datasetParams = NeuroBasicConditions.pointSelectionConfig[self.PointTypeName]\n",
      "        \n",
      "        ##class variable\n",
      "        NeuroPoints.XpointsofInterest = None\n",
      "        \n",
      "        self.runop()\n",
      "        \n",
      "        \n",
      "    def runop(self):\n",
      "        '''\n",
      "        These must correspond to the methods available in NeuroBasic Conditions under key\n",
      "        self.pointofinteresttype = [\"localmax\"]\n",
      "        '''\n",
      "        if (self.datasetParams[\"selection_algo\"] == \"localmax\" ):\n",
      "            NeuroPoints.XpointsofInterest = self.FindMaximalpos(self.datasetX,self.datasetY)\n",
      "            return True\n",
      "        \n",
      "        \n",
      "     # finds points of interest in dependant variable (phase points)\n",
      "    def FindMaximalpos(self,aX,aY):\n",
      "        '''\n",
      "        Input aligned:\n",
      "        x array\n",
      "        y array\n",
      "        \n",
      "        Output (X):\n",
      "        array[x_dto ,....]\n",
      "        \n",
      "        '''\n",
      "        maximaposX = []\n",
      "        length = len(aY)\n",
      "        if length >= 2:\n",
      "            if aY[0] > aY[1]:\n",
      "                maximaposX.append(aX[0])\n",
      "    \n",
      "           \n",
      "        if length > 3:\n",
      "            for i in range(1, length-1):     \n",
      "                if aY[i] > aY[i-1] and aY[i] > aY[i+1]:\n",
      "                    maximaposX.append(aX[i])\n",
      "    \n",
      "    \n",
      "        if aY[length-1] > aY[length-2]:\n",
      "            maximaposX.append(aX[length-1])\n",
      "           \n",
      "        #print \"maximaposX \" + str(maximaposX)\n",
      "        return maximaposX\n",
      "    \n",
      "    \n",
      "class NeuroCondSearch(object):\n",
      "    ''' \n",
      "    this class defines all the conditional operations available to be performed on normalized data\n",
      "    \n",
      "    '''\n",
      "\n",
      "    def __init__(self,datasetX,datasetY):\n",
      "        \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        \n",
      "    \n",
      "    def setID(self,ID): #id is the condition you want to search\n",
      "        #temporary lookup operation\n",
      "        conditionName = NeuroBasicConditions.idLookupCondtion[ID]\n",
      "        #assign constants\n",
      "        self.CursorShift = NeuroBasicConditions.conditionsConfig[conditionName]['shift']\n",
      "        self.CursorDelta = NeuroBasicConditions.conditionsConfig[conditionName]['delta']\n",
      "        #assign method\n",
      "        self.method = NeuroBasicConditions.conditionsConfig[conditionName]['condition']\n",
      "    \n",
      "    def runop(self,XSelectionPoint):\n",
      "        #print \"XSelectionPoint \" + str(XSelectionPoint) \n",
      "        self.XSelectionPoint = XSelectionPoint\n",
      "        \n",
      "        if (self.method == \"increase\"):\n",
      "            return self.Method_increase()\n",
      "    \n",
      "\n",
      "\n",
      "    '''\n",
      "    Methods for processing data, return is always boolean\n",
      "    \n",
      "    '''\n",
      "        \n",
      "    def Method_increase(self):\n",
      "               \n",
      "        #Phase point array\n",
      "        self.phase_shift_delta_sample_cursor()\n",
      "        return self.sectionincrease()\n",
      "        \n",
      "    '''\n",
      "    processing helpers\n",
      "    '''\n",
      "        \n",
      "    def phase_shift_delta_sample_cursor(self):\n",
      "        '''\n",
      "        Only two sets are computed using this\n",
      "        \n",
      "        index 0 used as the phase point, then delta searches backwards in time\n",
      "        self.CursorX = :\n",
      "        self.CursorY =\n",
      "        array[(x_dto,y)], array[(x_dto,y]\n",
      "        \n",
      "        '''\n",
      "        #shift\n",
      "        \n",
      "        #self.CursorShift\n",
      "        \n",
      "        #delta\n",
      "        #self.CursorDelta\n",
      "        \n",
      "        #phase\n",
      "        #print self.datasetX\n",
      "        #print \"==================================================================\"\n",
      "        #print self.XSelectionPoint\n",
      "        phaseIndex = self.datasetX.index(self.XSelectionPoint)\n",
      "        \n",
      "        shiftIndex = phaseIndex - self.CursorShift\n",
      "        \n",
      "        x0 = shiftIndex\n",
      "        x1 = shiftIndex - self.CursorDelta\n",
      "        x2 = x1 - self.CursorDelta\n",
      "        delta0 = []\n",
      "        delta1 = []\n",
      "        # both sets left to right, forward in time\n",
      "        for i in xrange(x1,x0):\n",
      "            delta0.append(self.datasetY[i])\n",
      "        for i in xrange(x2,x1):\n",
      "            delta1.append(self.datasetY[i])\n",
      "            \n",
      "        # left to right\n",
      "        self.CursorData = delta1,delta0 # output Y coords \n",
      "        return self\n",
      "            \n",
      "            \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "    '''\n",
      "    Micro operations\n",
      "    \n",
      "    '''\n",
      "    def sectionincrease(self): #only for comparing two sets\n",
      "        \n",
      "        '''\n",
      "        input section list[(tuple),(tuple)] \n",
      "        (tuple) = \n",
      "        \n",
      "        output tuple(x_dto,boolean)\n",
      "        \n",
      "        self.Cursor\n",
      "        \n",
      "        '''\n",
      "        #self.CursorData, left to right\n",
      "        #using average of list\n",
      "        av1 = reduce(lambda y1, y2: y1 + y2, self.CursorData[0]) / len(self.CursorData[0])\n",
      "        av2 = reduce(lambda y1, y2: y1 + y2, self.CursorData[1]) / len(self.CursorData[1])\n",
      "        if (av1<av2): # if left smaller than right, increased forward in time\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "        \n",
      "        \n",
      "    def allincreases(self):\n",
      "        '''\n",
      "        \n",
      "        input:\n",
      "        \n",
      "        \n",
      "        '''\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "import datetime\n",
      "from SecNeural import nn\n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "\n",
      "\n",
      "class NeuroBasicAlgo(NeuroBasicConditions,NeuroTrainer,NeuroPoints,NeuroCondSearch):\n",
      "    '''\n",
      "    ex:\n",
      "    algoConfig = {\"algo_id\" = \"SOME HEX\" , \"algoName\":\"Basic\", \"algoMode\":\"Basic\" \"independantVars\":[\"some var\",\"some var\"]  , \"dependantVars\":[\"some var\",\"some var\"]}\n",
      "    '''\n",
      "    def __init__(self,key,algoConfig):\n",
      "        self.key = key\n",
      "        self.algoConfig = algoConfig\n",
      "        self.parseconfig()\n",
      "        \n",
      "        \n",
      "        #creates conditional id subsystem\n",
      "        NeuroBasicConditions.__init__(self,self.key,self.dependantVars,self.independantVars) ## FINISH initialization\n",
      "        \n",
      "        NeuroTrainer.__init__(self) #init\n",
      "        self.build_neural_net() # pass neural database object to neural trainer\n",
      "        self.train_neural_net()\n",
      "\n",
      "        \n",
      "        \n",
      "    def parseconfig(self):\n",
      "        self.neuroStore = str(self.algoConfig[\"algo_id\"]) + \"_Basic.db\"\n",
      "        self.dependantVars = sorted(self.algoConfig[\"dependantVars\"], key=str.lower)\n",
      "        #independant variables always sorted alphabetically to ensure ID replication\n",
      "        self.independantVars = sorted(self.algoConfig[\"independantVars\"], key=str.lower)\n",
      "        return self\n",
      "        \n",
      "    def build_neural_net(self):\n",
      "        NeuroTrainer.mynet = nn.searchnet(self.neuroStore)\n",
      "        NeuroTrainer.mynet.maketables()\n",
      "        ####\n",
      "        '''\n",
      "        self.mynet.generatehiddennode( [   all condition ids for each analyzable dataset   ] , [  all point of interest ids   ] )\n",
      "        \n",
      "        self.mynet.generatehiddennode([dataids['dependant_data'],dataids['independant_data']],allconditionids(phasedeltas))\n",
      "        '''\n",
      "        \n",
      "        NeuroTrainer.mynet.generatehiddennode(NeuroBasicConditions.allPointSelectionIds,NeuroBasicConditions.allConditionIds)\n",
      "        return self\n",
      "           \n",
      "    def train_neural_net(self):\n",
      "        #aquire data\n",
      "            NeuroTrainer.aquiredata(self)\n",
      "            NeuroTrainer.methodBasicTrain(self)\n",
      "    \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "algoConfig ={\"algo_id\" : \"hx0\" , \"algoName\":\"Basic\", \"algoMode\":\"Basic\", \"independantVars\":[\"independant_question\"]  , \"dependantVars\":[\"dependant_question\"]}\n",
      "key = \"\"\n",
      "time1 = datetime.datetime.now()\n",
      "test = NeuroBasicAlgo(key,algoConfig)\n",
      "time2 = datetime.datetime.now()\n",
      "tottime = time2 - time1\n",
      "print \"EXCECUTION TIME: \" + str(tottime)\n",
      "print \"S: \" + str(tottime.seconds) + \" us: \" + str(tottime.microseconds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-07-25 00:00:00\n",
        "2014-08-13 00:00:00\n",
        "2014-07-24 00:30:00\n",
        "2014-08-11 20:22:43\n",
        "EXCECUTION TIME: 0:00:05.953000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "S: 5 us: 953000\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Network Level 1 data retreival"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hx0_Basic.db"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "##### builder test class\n",
      "\n",
      "'''\n",
      "class testconditionbuilder(NeuroBasicConditions):\n",
      "    def __init__(self):\n",
      "        depVarsLst = ['dependant_question']\n",
      "        indepVarsLst = ['independant_question']\n",
      "        key =  \"\"\n",
      "        NeuroBasicConditions.__init__(self,key,depVarsLst,indepVarsLst)\n",
      "        \n",
      "        \n",
      "test = testconditionbuilder()\n",
      "\n",
      "#test.conditionsConfig\n",
      "#test.baseconditions\n",
      "\n",
      "\n",
      "#test.pointofinteresttype\n",
      "\n",
      "#test.phasedeltas \n",
      "\n",
      "#test.phaseshifts\n",
      "\n",
      "#test.conditionsConfig \n",
      "#test.conditionLookupId\n",
      "#test.idLookupCondtion\n",
      "#test.allConditionIds\n",
      "#test.DatasetConditions\n",
      "\n",
      "#test.pointSelectionConfig \n",
      "#test.pointSelectionLookupId\n",
      "#test.idLookupPointSelection\n",
      "#test.allPointSelectionIds\n",
      "\n",
      "#test.datasetConfig \n",
      "#test.datasetLookupId\n",
      "#test.idLookupDataset\n",
      "#test.allDatasetIds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print test.allPointSelectionIds\n",
      "\n",
      "print test.allConditionIds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[300]\n",
        "[700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SecNeural import nn\n",
      "\n",
      "\n",
      "mynet = nn.searchnet('hx0_Basic.db')\n",
      "mynet.getresult(test.allPointSelectionIds,test.allConditionIds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[5.808843478391672e-05,\n",
        " 9.103077913493837e-05,\n",
        " 0.00013617779371593398,\n",
        " 0.0005189249151211695,\n",
        " 0.0011007267824873147,\n",
        " 0.0019291568610005996,\n",
        " 0.0020196075507779685,\n",
        " 0.0027283681845566574,\n",
        " 0.0022792077261355333,\n",
        " 0.0013135925947854417,\n",
        " 0.0010957446740613233,\n",
        " 0.001074928518634031,\n",
        " 0.0010282778623403543,\n",
        " 0.0009777622922643064,\n",
        " 0.0009248492653986683,\n",
        " 0.0008710122782880223,\n",
        " 0.0008172839867342367,\n",
        " 0.0005742475050510542,\n",
        " 0.0005453995617835536,\n",
        " 0.0005100658121617948,\n",
        " 0.000476887957570232,\n",
        " 0.00044587807614492915,\n",
        " 0.0004170905184619091,\n",
        " 0.0003903018444852423,\n",
        " 0.00036548185975449434,\n",
        " 0.00025746359046072793,\n",
        " 0.0002468169635631728,\n",
        " 0.00023374268825533648,\n",
        " 0.00022167691355844005,\n",
        " 0.00021063171734529853,\n",
        " 0.00020047424325366964,\n",
        " 0.00014373852965587837,\n",
        " 0.00014114179127575774,\n",
        " 0.00010303012615202891,\n",
        " 7.779949107527244e-05,\n",
        " 0.00010599523907431036,\n",
        " 0.00010461836382753252,\n",
        " 0.00010356759061261912,\n",
        " 0.00010261947914839175,\n",
        " 7.654339435246477e-05,\n",
        " 7.883818644202429e-05,\n",
        " 6.019602015572989e-05,\n",
        " 4.793096029165673e-05,\n",
        " 3.939191811266062e-05,\n",
        " 5.8861417384528027e-05,\n",
        " 6.166347931114758e-05,\n",
        " 6.433872377897842e-05,\n",
        " 5.025594702240326e-05,\n",
        " 5.448319562235105e-05,\n",
        " 5.779252738210178e-05,\n",
        " 4.57569467247751e-05,\n",
        " 2.8050813993555066e-05,\n",
        " 2.684302867688005e-05,\n",
        " 4.345611569935469e-05,\n",
        " 3.58531071371492e-05,\n",
        " 3.097365445956305e-05,\n",
        " 2.757977772006135e-05,\n",
        " 2.521855742582929e-05,\n",
        " 2.3575969394891728e-05,\n",
        " 2.2428573343793167e-05,\n",
        " 2.1625396107988858e-05,\n",
        " 2.106981486215295e-05,\n",
        " 3.637849374961339e-05,\n",
        " 4.119755715951799e-05,\n",
        " 3.436149227177042e-05,\n",
        " 2.9934959087403363e-05,\n",
        " 2.6849067603463622e-05,\n",
        " 2.4711287592759744e-05,\n",
        " 2.3219672726399207e-05,\n",
        " 2.2174938427226462e-05,\n",
        " 3.7725174376871716e-05,\n",
        " 5.621032862120654e-05]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.unlink('hx0_Basic.db')\n",
      "os.remove('hx0_Basic.db')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "WindowsError",
       "evalue": "[Error 32] The process cannot access the file because it is being used by another process: 'hx0_Basic.db'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mWindowsError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-7f4e30461d2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hx0_Basic.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hx0_Basic.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mWindowsError\u001b[0m: [Error 32] The process cannot access the file because it is being used by another process: 'hx0_Basic.db'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "from SecApp.DBQuestionClass import SecuQ\n",
      "class NeuroDataComparative:\n",
      "    '''\n",
      "    REQUIRES NoSQL DATABASE ACCESS KEY\n",
      "    Establishes IDs for all data needing to be examined. \n",
      "    \n",
      "    For use in comparing many variables against one dependant\n",
      "    \n",
      "    conditions:\n",
      "    only one dependant variable\n",
      "    \n",
      "    as many independants as you'd like (must be list)\n",
      "    '''\n",
      "    \n",
      "    def __init__(self,key,depVar,indepVarsLst):\n",
      "        self.key = key\n",
      "        initQ = SecuQ(self.key)\n",
      "        \n",
      "        self.variable_v2id_dict = {}\n",
      "        self.variable_id2v_dict = {}\n",
      "        self.variable_lst = []\n",
      "        self.variableids_lst = []\n",
      "        \n",
      "        \n",
      "        self.independantVars = sorted(indepVarsLst, key=str.lower)\n",
      "        self.dependantVar = depVar\n",
      "        self.varTypes_v2t_dict = {}\n",
      "        self.varTypes_id2t_dict = {}\n",
      "        # data IDs start at 900, 900 is reserved for dependant variable, others are after that\n",
      "        \n",
      "        \n",
      "        #class main loop\n",
      "        self.iterateVariables()\n",
      "        \n",
      "        \n",
      "        \n",
      "    def genVariableids(self,varkey):\n",
      "            \n",
      "        self.variable_v2id_dict[varkey] = idcounter\n",
      "        self.variable_id2v_dict[idcounter] = varkey\n",
      "        self.variable_lst.append(varkey)\n",
      "        self.variableids_lst.append(idcounter)\n",
      "        self.idcounter +=1\n",
      "        return self\n",
      "    \n",
      "    #This determines the timedelta or normalization technique\n",
      "    def getanalyticstype(self,varkey): \n",
      "        if (varkey in initQ.multipoint.keys()) & (varkey in initQ.aggregate.keys()):\n",
      "            #self.varTypes_v2t_dict[varkey] = 'mp_aggre' # multipoint with aggregate data in dailykey, \"keyname\"_aggregate:value\n",
      "            \n",
      "            #self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'mp_aggre'\n",
      "            raise Exception(\"NO MULTI-AGREGATE VARIABLES (IN ALPHA)\")\n",
      "            return False #breakloop\n",
      "            \n",
      "        if (varkey in initQ.aggregate.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'aggregate' # aggregate data in dailykey\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'aggregate'\n",
      "            return self #breakloop \n",
      "        \n",
      "        if (varkey in initQ.multipoint.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'multipoint'\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'multipoint'\n",
      "            return self #breakloop\n",
      "            \n",
      "        if (varkey in initQ.active.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'daily' # data in dailykey\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'daily'\n",
      "            return self #breakloop\n",
      "\n",
      "        \n",
      "    def iterateVariables(self):\n",
      "        #dependant first\n",
      "        self.idcounter = 900\n",
      "        self.genVariableids(self.dependantVar)\n",
      "        self.getanalyticstype(self.dependantVar)\n",
      "        \n",
      "        #independants in alphabetical order\n",
      "        for var in self.independantVars:\n",
      "            self.genVariableids(var)\n",
      "            self.getanalyticstype(var)\n",
      "            \n",
      "        return self\n",
      "        \n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}