{
 "metadata": {
  "name": "",
  "signature": "sha256:dd836fea53642d19eb16a22ee86b6eed7e195cf74e382d15ab6f5759b0def9ad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "data normalizer ref:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#For arbitrary date, daily point datastructures, averages points that are non existing\n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "import datetime\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "initDO = SecuFrame('')\n",
      "\n",
      "timeObjs = initDO.iffo().orderedmap().processFrameList\n",
      "keyOfInterest = 'dependant_question'\n",
      "normConst = datetime.timedelta(minutes = 20 )\n",
      "\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalized_dtokey_dict\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalizedX\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalizedY"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "FULL TEST CLASS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "algoConfig ={\"algo_id\" : \"hx0\" , \"algoName\":\"Basic\", \"algoMode\":\"Basic\", \"independantVars\":[\"independant_question\"]  , \"dependantVars\":[\"dependant_question\"]}\n",
      "key = \"\"\n",
      "test = NeuroBasicAlgo(key,algoConfig)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OperationalError",
       "evalue": "table hiddennode already exists",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-a24f97a1d925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0malgoConfig\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"algo_id\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"hx0\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"algoName\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"Basic\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"algoMode\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"Basic\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"independantVars\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"independant_question\"\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;34m\"dependantVars\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dependant_question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuroBasicAlgo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgoConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-7-71136c1adfc1>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, key, algoConfig)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mNeuroTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#init\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_neural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# pass neural database object to neural trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-7-71136c1adfc1>\u001b[0m in \u001b[0;36mbuild_neural_net\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_neural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mNeuroTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmynet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneuroStore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mNeuroTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmynet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         '''\n",
        "\u001b[1;32mC:\\Python27\\scripts\\SecNeural\\nn.pyc\u001b[0m in \u001b[0;36mmaketables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmaketables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'create table hiddennode(create_key)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'create table wordhidden(fromid,toid,strength)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'create table hiddenurl(fromid,toid,strength)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mOperationalError\u001b[0m: table hiddennode already exists"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Algo Parser Selector class "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "from SecNeural import nn\n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "\n",
      "\n",
      "class NeuroBasicAlgo(NeuroBasicConditions,NeuroTrainer,NeuroPoints,NeuroCondSearch):\n",
      "    '''\n",
      "    ex:\n",
      "    algoConfig = {\"algo_id\" = \"SOME HEX\" , \"algoName\":\"Basic\", \"algoMode\":\"Basic\" \"independantVars\":[\"some var\",\"some var\"]  , \"dependantVars\":[\"some var\",\"some var\"]}\n",
      "    '''\n",
      "    def __init__(self,key,algoConfig):\n",
      "        self.key = key\n",
      "        self.algoConfig = algoConfig\n",
      "        self.parseconfig()\n",
      "        \n",
      "        \n",
      "        #creates conditional id subsystem\n",
      "        NeuroBasicConditions.__init__(self,self.key,self.dependantVars,self.independantVars) ## FINISH initialization\n",
      "        \n",
      "        NeuroTrainer.__init__(self) #init\n",
      "        self.build_neural_net() # pass neural database object to neural trainer\n",
      "\n",
      "\n",
      "        \n",
      "        \n",
      "    def parseconfig(self):\n",
      "        self.neuroStore = str(self.algoConfig[\"algo_id\"]) + \"_Basic.db\"\n",
      "        self.dependantVars = sorted(self.algoConfig[\"dependantVars\"], key=str.lower)\n",
      "        #independant variables always sorted alphabetically to ensure ID replication\n",
      "        self.independantVars = sorted(self.algoConfig[\"independantVars\"], key=str.lower)\n",
      "        return self\n",
      "        \n",
      "    def build_neural_net(self):\n",
      "        NeuroTrainer.mynet = nn.searchnet(self.neuroStore)\n",
      "        NeuroTrainer.mynet.maketables()\n",
      "        ####\n",
      "        '''\n",
      "        self.mynet.generatehiddennode( [   all condition ids for each analyzable dataset   ] , [  all point of interest ids   ] )\n",
      "        \n",
      "        self.mynet.generatehiddennode([dataids['dependant_data'],dataids['independant_data']],allconditionids(phasedeltas))\n",
      "        '''\n",
      "        \n",
      "        NeuroTrainer.mynet.generatehiddennode(NeuroBasicConditions.allConditionIds, NeuroBasicConditions.allPointSelectionIds)\n",
      "        return self\n",
      "           \n",
      "    def train_neural_net(self):\n",
      "        #aquire data\n",
      "        initDO = SecuFrame(self.key)\n",
      "        timeObjs = initDO.iffo().orderedmap().processFrameList\n",
      "        \n",
      "        for variable in NDC.variable_lst:\n",
      "      \n",
      "            normConst = determineNormConstant(variable) \n",
      "            ND = DataNormalization(timeObjs,variable,normConst)\n",
      "            '''\n",
      "            ND.normalized_dtokey_dict = {}\n",
      "            ND.normalized = []\n",
      "            ND.normalized_dto = []\n",
      "            \n",
      "            ND.normalizedX = []\n",
      "            ND.normalizedX_dto = []\n",
      "            ND.normalizedY = []\n",
      "            '''\n",
      "            \n",
      "            \n",
      "            ## pass data to trainer\n",
      "            #---- trainer to and from conditions\n",
      "            \n",
      "            \n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Training main class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "import datetime\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "\n",
      "\n",
      "class NeuroTrainer(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        #pass mynet object\n",
      "        NeuroTrainer.mynet = None\n",
      "\n",
      "        \n",
      "        \n",
      "    def aquiredata(self):\n",
      "        self.initDO = SecuFrame('')\n",
      "        self.fromDB = initDO.iffo().orderedmap().processFrameList\n",
      "        return self\n",
      "    \n",
      "    def methodBasicTrain(self):\n",
      "        for selectionPointVariable in NeuroBasicConditions.pointSelectionConfig.keys(): # iterate through point sets\n",
      "            \n",
      "            #Normalized points dataset(object)\n",
      "            normalizedSelectionPointData = DataNormalization(self.fromDB,selectionPointVariable,NeuroBasicConditions.pointselectionConfig[selectionPointVariable][\"NormConstant\"])\n",
      "            normalizedSelectionPointDataID = NeuroBasicConditions.pointSelectionConfig[selectionPointVariable][\"ID\"]\n",
      "            #computed selection points\n",
      "            \n",
      "            NeuroPoints.__init__(self,normalizedSelectionPointDataID,normalizedSelectionPointData.normalizedX,normalizedSelectionPointData.normalizedY)\n",
      "            computedSelectionPoints = NeuroPoints.XpointsofInterest\n",
      "            for analyzableDatasetVariable in NeuroBasicConditions.datasetConfig.keys(): # iterate through datasets\n",
      "                \n",
      "                #Normalize dataset (object)\n",
      "                normalizedAnalyzableDataset = DataNormalization(self.fromDB,analyzableDatasetVariable,NeuroBasicConditions.datasetConfig[analyzableDatasetVariable][\"NormConstant\"])\n",
      "                normalizedAnalyzableDatasetID = NeuroBasicConditions.datasetConfig[analyzableDatasetVariable][\"ID\"]\n",
      "                #compute each condition and train neural network\n",
      "                \n",
      "                \n",
      "                NeuroCondSearch.__init__(self, normalizedAnalyzableDataset.normalizedX, normalizedAnalyzableDataset.normalizedY)\n",
      "                \n",
      "                for computeConditionID in NeuroBasicConditions.DatasetConditions[normalizedAnalyzableDatasetID]: # returns ids of conditions attached to dataset\n",
      "                    \n",
      "                    #initialize conditional search, use id\n",
      "                    NeuroCondSearch.setID(self, computeConditionID)\n",
      "                    \n",
      "                    \n",
      "                    for xpoint in computedSelectionPoints: # iterate through each point\n",
      "                        bool_result = NeuroCondSearch.runop(xpoint)\n",
      "                        #Train neuralNet\n",
      "                        \n",
      "                        #Level 1\n",
      "                        #mynet.trainquery([words],[urls],url clicked)\n",
      "                        #mynet.trainquery([conditions],[point of interest method + set], point of interest method ok)\n",
      "                        '''\n",
      "                        TEST TYPE:\n",
      "                        query conditions, output selection point id\n",
      "                        \n",
      "                        invert map:\n",
      "                        \n",
      "                        query selection point sets, output conditions\n",
      "                        '''\n",
      "                        if (bool_result) :\n",
      "                            NeuroTrainer.mynet.trainquery(NeuroBasicConditions.allConditionIds, NeuroBasicConditions.allPointSelectionIds, normalizedSelectionPointDataID)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Algo Automated search condition builder"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SecApp.DBQuestionClass import SecuQ\n",
      "class NeuroBasicConditions(object):\n",
      "    def __init__(self,key,depVarsLst,indepVarsLst):\n",
      "        self.depVarsLst = depVarsLst\n",
      "        self.indepVarsLst = indepVarsLst\n",
      "        self.key = key\n",
      "        self.initQ = SecuQ(self.key)\n",
      "        '''\n",
      "        available parameters used to construct conditions\n",
      "        '''\n",
      "        #analytics conditions\n",
      "        self.baseconditions = [\"increase\"]\n",
      "        self.pointofinteresttype = [\"localmax\"]\n",
      "        #more to be added\n",
      "        #baseconditions = {\"increase\":1,\"decrease\":2,\"MA14_increase\":3,\"MA14_decrease\":4}\n",
      "        self.phasedeltas = [2,3,4,5,6,7,14,21] #how much data in each segment \n",
      "        self.phaseshifts = [1,2,3,4,5,6,7,14,21] # how much data is shifted from phase ( x coord[date])\n",
      "    \n",
      "        '''\n",
      "        constructed conditions (with different maps for easy access)\n",
      "        '''\n",
      "\n",
      "    \n",
      "        NeuroBasicConditions.conditionsConfig = {} #analysis algo conditions\n",
      "        NeuroBasicConditions.conditionLookupId = {}\n",
      "        NeuroBasicConditions.idLookupCondtion = {}\n",
      "        NeuroBasicConditions.allConditionIds = []\n",
      "        NeuroBasicConditions.DatasetConditions = {}\n",
      "        \n",
      "    \n",
      "    \n",
      "        NeuroBasicConditions.pointSelectionConfig = {} #dependant IDs point selection algo\n",
      "        NeuroBasicConditions.pointSelectionLookupId = {}\n",
      "        NeuroBasicConditions.idLookupPointSelection = {}\n",
      "        NeuroBasicConditions.allPointSelectionIds = []\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "        NeuroBasicConditions.datasetConfig = {} #independantID datasets available\n",
      "        NeuroBasicConditions.datasetLookupId = {}\n",
      "        NeuroBasicConditions.idLookupDataset = {}\n",
      "        NeuroBasicConditions.allDatasetIds = []\n",
      "        \n",
      "    \n",
      "        '''\n",
      "        this holds starting id numbers for configurtation data \n",
      "        '''\n",
      "        ###\n",
      "        self.depIDcount = 300\n",
      "        \n",
      "        self.indepIDcount = 500\n",
      "        \n",
      "        self.condIDcount = 700\n",
      "    \n",
      "        self.buildselectionpointIDs()\n",
      "        self.buildindependantIDs()\n",
      "        self.buildconditions()\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "    def getNormConstant(self,varkey):  # for variables only\n",
      "        if (varkey in self.initQ.multipoint.keys()) & (varkey in self.initQ.aggregate.keys()):\n",
      "            #self.varTypes_v2t_dict[varkey] = 'mp_aggre' # multipoint with aggregate data in dailykey, \"keyname\"_aggregate:value\n",
      "            \n",
      "            #self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'mp_aggre'\n",
      "            raise Exception(\"NO MULTI-AGREGATE VARIABLES (IN ALPHA)\")\n",
      "            return False #breakloop\n",
      "            \n",
      "        if (varkey in self.initQ.aggregate.keys()):\n",
      "            return datetime.timedelta(days = 1 )\n",
      "        \n",
      "        if (varkey in self.initQ.multipoint.keys()):\n",
      "            return datetime.timedelta(minutes = 30 )\n",
      "            \n",
      "        if (varkey in self.initQ.active.keys()):\n",
      "            return datetime.timedelta(days = 1 )\n",
      "  \n",
      "    \n",
      "    \n",
      "    def buildselectionpointIDs(self): #builddependantIDs\n",
      "        for pointselectionalgo in self.pointofinteresttype:\n",
      "            for var in self.depVarsLst:\n",
      "                \n",
      "                builtPointSelectionTmp = str(var) + \"-\" + str(pointselectionalgo)+ \"-\" + str(self.depIDcount)\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp] = {}\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"pointSelectSet\"] = var\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"selection_algo\"] = pointselectionalgo\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"ID\"] = self.depIDcount\n",
      "                NeuroBasicConditions.pointSelectionConfig[builtPointSelectionTmp][\"NormConstant\"] = self.getNormConstant(var)\n",
      "              \n",
      "                NeuroBasicConditions.pointSelectionLookupId[builtPointSelectionTmp] = self.depIDcount\n",
      "            \n",
      "                NeuroBasicConditions.idLookupPointSelection[self.depIDcount] = builtPointSelectionTmp\n",
      "            \n",
      "                NeuroBasicConditions.allPointSelectionIds.append(self.depIDcount)\n",
      "            \n",
      "                self.depIDcount+= 1\n",
      "        return self\n",
      "            \n",
      "                \n",
      "            \n",
      "            \n",
      "    def buildindependantIDs(self): #build analyzable (comparable dataset) ids\n",
      "        for var in self.indepVarsLst:\n",
      "            NeuroBasicConditions.datasetConfig[var] = {}\n",
      "            NeuroBasicConditions.datasetConfig[var][\"ID\"] = self.indepIDcount\n",
      "            NeuroBasicConditions.datasetConfig[var][\"NormConstant\"] = self.getNormConstant(var)\n",
      "            \n",
      "            NeuroBasicConditions.datasetLookupId[var] = self.indepIDcount\n",
      "            \n",
      "            NeuroBasicConditions.idLookupDataset[self.indepIDcount] = var\n",
      "            \n",
      "            NeuroBasicConditions.allDatasetIds.append(self.indepIDcount)\n",
      "            \n",
      "            \n",
      "            self.indepIDcount+=1\n",
      "        return self\n",
      "\n",
      "    \n",
      "    def buildconditions(self): # add [haseshifts]\n",
      "        for var in NeuroBasicConditions.datasetConfig.keys():\n",
      "            currVarID = NeuroBasicConditions.datasetConfig[var][\"ID\"]\n",
      "            NeuroBasicConditions.DatasetConditions[currVarID] = []\n",
      "            \n",
      "            for condition in self.baseconditions:\n",
      "                for delta in self.phasedeltas:\n",
      "                    for shift in self.phaseshifts:\n",
      "                        \n",
      "                        \n",
      "                    \n",
      "                        builtconditiontmp = str(condition) + \"-\" + str(currVarID)+ \"-\" + str(delta)+ \"-\" + str(shift)\n",
      "                    \n",
      "                        \n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp] = {}\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp][\"analyze\"] = currVarID\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['condition'] = condition\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['delta'] = delta\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['shift'] = shift\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['ID'] = self.condIDcount\n",
      "                        \n",
      "                        \n",
      "                        NeuroBasicConditions.DatasetConditions[currVarID].append(self.condIDcount)\n",
      "                        \n",
      "                        NeuroBasicConditions.idLookupCondtion[self.condIDcount] = builtconditiontmp\n",
      "                    \n",
      "                        NeuroBasicConditions.conditionLookupId[builtconditiontmp] = self.condIDcount\n",
      "                    \n",
      "                        NeuroBasicConditions.allConditionIds.append(self.condIDcount)\n",
      "                    \n",
      "                        self.condIDcount+=1\n",
      "        return self\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Interest point finder"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Finds the points that \n",
      "-repeat \n",
      "-are anomalies within the set that should be looked at"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuroPoints(object):\n",
      "    def __init__(self,ID,datasetX,datasetY):      \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        self.datasetID = ID\n",
      "        \n",
      "        ## must be initialized before hand\n",
      "        self.PointTypeName = NeuroBasicConditions.idLoockupPointSelection[self.datasetID]\n",
      "        self.datasetParams = NeuroBasicConditions.pointSelectionConfig[self.PointTypeName]\n",
      "        \n",
      "        ##class variable\n",
      "        NeuroPoints.XpointsofInterest = None\n",
      "        \n",
      "        self.runop()\n",
      "        \n",
      "        \n",
      "    def runop(self):\n",
      "        '''\n",
      "        These must correspond to the methods available in NeuroBasic Conditions under key\n",
      "        self.pointofinteresttype = [\"localmax\"]\n",
      "        '''\n",
      "        if (self.datasetParams[\"selection_algo\"] == \"localmax\" ):\n",
      "            NeuroPoints.XpointsofInterest = self.FindMaximalpos(self.datasetX,self.datasetY)\n",
      "            return True\n",
      "        \n",
      "        \n",
      "     # finds points of interest in dependant variable (phase points)\n",
      "    def FindMaximalpos(self,aX,aY):\n",
      "        '''\n",
      "        Input aligned:\n",
      "        x array\n",
      "        y array\n",
      "        \n",
      "        Output (X):\n",
      "        array[x_dto ,....]\n",
      "        \n",
      "        '''\n",
      "        maximaposX = []\n",
      "        length = len(aY)\n",
      "        if length >= 2:\n",
      "            if aY[0] > aY[1]:\n",
      "                maximaposX.append(0)\n",
      "    \n",
      "           \n",
      "        if length > 3:\n",
      "            for i in range(1, length-1):     \n",
      "                if aY[i] > aY[i-1] and aY[i] > aY[i+1]:\n",
      "                    maximaposX.append(i)\n",
      "    \n",
      "    \n",
      "        if aY[length-1] > aY[length-2]:\n",
      "            maximaposX.append(length-1)\n",
      "           \n",
      "        return maximapos\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Conditional search, pattern determined by builder configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuroCondSearch(object):\n",
      "    ''' \n",
      "    this class defines all the conditional operations available to be performed on normalized data\n",
      "    \n",
      "    '''\n",
      "\n",
      "    def __init__(self,datasetX,datasetY):\n",
      "        \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        \n",
      "    \n",
      "    def setID(self,ID): #id is the condition you want to search\n",
      "        #temporary lookup operation\n",
      "        conditionName = NeuroBasicConditions.idLookupCondtion[datasetID]\n",
      "        #assign constants\n",
      "        self.CursorShift = NeuroBasicConditions.conditionsConfig[conditionName]['shift']\n",
      "        self.CursorDelta = NeuroBasicConditions.conditionsConfig[conditionName]['delta']\n",
      "        #assign method\n",
      "        self.method = NeuroBasicConditions.conditionsConfig[conditionName]['condition']\n",
      "    \n",
      "    def runop(self,XSelectionPoint):\n",
      "        \n",
      "        self.XSelectionPoint = XSelectionPoint\n",
      "        \n",
      "        if (self.method == \"increase\"):\n",
      "            return self.Method_increase()\n",
      "    \n",
      "\n",
      "\n",
      "    '''\n",
      "    Methods for processing data, return is always boolean\n",
      "    \n",
      "    '''\n",
      "        \n",
      "    def Method_increase(self):\n",
      "        \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        \n",
      "        #Phase point array\n",
      "        self.phase_shift_delta_sample_cursor()\n",
      "        return self.sectionincrease()\n",
      "        \n",
      "    '''\n",
      "    processing helpers\n",
      "    '''\n",
      "        \n",
      "    def phase_shift_delta_sample_cursor(self):\n",
      "        '''\n",
      "        Only two sets are computed using this\n",
      "        \n",
      "        index 0 used as the phase point, then delta searches backwards in time\n",
      "        self.CursorX = :\n",
      "        self.CursorY =\n",
      "        array[(x_dto,y)], array[(x_dto,y]\n",
      "        \n",
      "        '''\n",
      "        #shift\n",
      "        #self.CursorShift\n",
      "        \n",
      "        #delta\n",
      "        #self.CursorDelta\n",
      "        \n",
      "        #phase\n",
      "        phaseIndex = self.datasetX.index(self.XSelectionPoint)\n",
      "        \n",
      "        shiftIndex = phaseIndex - self.CursorShift\n",
      "        \n",
      "        x0 = shiftIndex\n",
      "        x1 = shiftIndex - self.CursorDelta\n",
      "        x2 = x1 - self.CursorDelta\n",
      "        delta0 = []\n",
      "        delta1 = []\n",
      "        # both sets left to right, forward in time\n",
      "        for i in xrange(x1,x0):\n",
      "            delta0.append(self.datasetY[i])\n",
      "        for i in xrange(x2,x1):\n",
      "            delta1.append(self.datasetY[i])\n",
      "            \n",
      "        # left to right\n",
      "        self.CursorData = delta1,delta0 # output Y coords \n",
      "        return self\n",
      "            \n",
      "            \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "    '''\n",
      "    Micro operations\n",
      "    \n",
      "    '''\n",
      "    def sectionincrease(self): #only for comparing two sets\n",
      "        \n",
      "        '''\n",
      "        input section list[(tuple),(tuple)] \n",
      "        (tuple) = \n",
      "        \n",
      "        output tuple(x_dto,boolean)\n",
      "        \n",
      "        self.Cursor\n",
      "        \n",
      "        '''\n",
      "        #self.CursorData, left to right\n",
      "        #using average of list\n",
      "        av1 = reduce(lambda y1, y2: y1 + y2, self.CursorData[0]) / len(self.CursorData[0])\n",
      "        av2 = reduce(lambda y1, y2: y1 + y2, self.CursorData[1]) / len(self.CursorData[1])\n",
      "        if (av1<av2): # if left smaller than right, increased forward in time\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "        \n",
      "        \n",
      "    def allincreases(self):\n",
      "        '''\n",
      "        \n",
      "        input:\n",
      "        \n",
      "        \n",
      "        '''\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Network Level 1 data retreival"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hx0_Basic.db"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "##### builder test class\n",
      "\n",
      "'''\n",
      "class testconditionbuilder(NeuroBasicConditions):\n",
      "    def __init__(self):\n",
      "        depVarsLst = ['dependant_question']\n",
      "        indepVarsLst = ['independant_question']\n",
      "        key =  \"\"\n",
      "        NeuroBasicConditions.__init__(self,key,depVarsLst,indepVarsLst)\n",
      "        \n",
      "        \n",
      "test = testconditionbuilder()\n",
      "\n",
      "#test.conditionsConfig\n",
      "#test.baseconditions\n",
      "\n",
      "\n",
      "#test.pointofinteresttype\n",
      "\n",
      "#test.phasedeltas \n",
      "\n",
      "#test.phaseshifts\n",
      "\n",
      "#test.conditionsConfig \n",
      "#test.conditionLookupId\n",
      "#test.idLookupCondtion\n",
      "#test.allConditionIds\n",
      "#test.DatasetConditions\n",
      "\n",
      "#test.pointSelectionConfig \n",
      "#test.pointSelectionLookupId\n",
      "#test.idLookupPointSelection\n",
      "#test.allPointSelectionIds\n",
      "\n",
      "#test.datasetConfig \n",
      "#test.datasetLookupId\n",
      "#test.idLookupDataset\n",
      "#test.allDatasetIds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SecNeural import nn\n",
      "\n",
      "\n",
      "mynet = nn.searchnet('hx0_Basic.db')\n",
      "mynet.getresult(test.allConditionIds, test.allPointSelectionIds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[0.0]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "from SecApp.DBQuestionClass import SecuQ\n",
      "class NeuroDataComparative:\n",
      "    '''\n",
      "    REQUIRES NoSQL DATABASE ACCESS KEY\n",
      "    Establishes IDs for all data needing to be examined. \n",
      "    \n",
      "    For use in comparing many variables against one dependant\n",
      "    \n",
      "    conditions:\n",
      "    only one dependant variable\n",
      "    \n",
      "    as many independants as you'd like (must be list)\n",
      "    '''\n",
      "    \n",
      "    def __init__(self,key,depVar,indepVarsLst):\n",
      "        self.key = key\n",
      "        initQ = SecuQ(self.key)\n",
      "        \n",
      "        self.variable_v2id_dict = {}\n",
      "        self.variable_id2v_dict = {}\n",
      "        self.variable_lst = []\n",
      "        self.variableids_lst = []\n",
      "        \n",
      "        \n",
      "        self.independantVars = sorted(indepVarsLst, key=str.lower)\n",
      "        self.dependantVar = depVar\n",
      "        self.varTypes_v2t_dict = {}\n",
      "        self.varTypes_id2t_dict = {}\n",
      "        # data IDs start at 900, 900 is reserved for dependant variable, others are after that\n",
      "        \n",
      "        \n",
      "        #class main loop\n",
      "        self.iterateVariables()\n",
      "        \n",
      "        \n",
      "        \n",
      "    def genVariableids(self,varkey):\n",
      "            \n",
      "        self.variable_v2id_dict[varkey] = idcounter\n",
      "        self.variable_id2v_dict[idcounter] = varkey\n",
      "        self.variable_lst.append(varkey)\n",
      "        self.variableids_lst.append(idcounter)\n",
      "        self.idcounter +=1\n",
      "        return self\n",
      "    \n",
      "    #This determines the timedelta or normalization technique\n",
      "    def getanalyticstype(self,varkey): \n",
      "        if (varkey in initQ.multipoint.keys()) & (varkey in initQ.aggregate.keys()):\n",
      "            #self.varTypes_v2t_dict[varkey] = 'mp_aggre' # multipoint with aggregate data in dailykey, \"keyname\"_aggregate:value\n",
      "            \n",
      "            #self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'mp_aggre'\n",
      "            raise Exception(\"NO MULTI-AGREGATE VARIABLES (IN ALPHA)\")\n",
      "            return False #breakloop\n",
      "            \n",
      "        if (varkey in initQ.aggregate.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'aggregate' # aggregate data in dailykey\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'aggregate'\n",
      "            return self #breakloop \n",
      "        \n",
      "        if (varkey in initQ.multipoint.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'multipoint'\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'multipoint'\n",
      "            return self #breakloop\n",
      "            \n",
      "        if (varkey in initQ.active.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'daily' # data in dailykey\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'daily'\n",
      "            return self #breakloop\n",
      "\n",
      "        \n",
      "    def iterateVariables(self):\n",
      "        #dependant first\n",
      "        self.idcounter = 900\n",
      "        self.genVariableids(self.dependantVar)\n",
      "        self.getanalyticstype(self.dependantVar)\n",
      "        \n",
      "        #independants in alphabetical order\n",
      "        for var in self.independantVars:\n",
      "            self.genVariableids(var)\n",
      "            self.getanalyticstype(var)\n",
      "            \n",
      "        return self\n",
      "        \n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}