{
 "metadata": {
  "name": "",
  "signature": "sha256:0e8407613a20fb6a4549e19ec82fdea368e954a43c6190b47f9fb096c287fca4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#For arbitrary date, daily point datastructures, averages points that are non existing\n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "import datetime\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "initDO = SecuFrame('')\n",
      "\n",
      "timeObjs = initDO.iffo().orderedmap().processFrameList\n",
      "keyOfInterest = 'dependant_question'\n",
      "normConst = datetime.timedelta(minutes = 20 )\n",
      "\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalized_dtokey_dict\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalizedX\n",
      "DataNormalization(timeObjs,keyOfInterest,normConst).normalizedY"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "from SecNeural import nn\n",
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "\n",
      "\n",
      "class NeuroBasicAlgo:\n",
      "    '''\n",
      "    ex:\n",
      "    algoConfig = {\"algo_id\" = \"SOME HEX\" , \"algoName\":\"Basic\", \"algoMode\":\"Basic\" \"independantVars\":[\"some var\",\"some var\"]  , \"dependantVars\":[\"some var\",\"some var\"]}\n",
      "    '''\n",
      "    def __init__(self,key,algoConfig):\n",
      "        self.key = key\n",
      "        self.algoConfig = algoConfig\n",
      "        self.parseconfig()\n",
      "        self.build_neural_net()\n",
      "        \n",
      "        #creates conditional id subsystem\n",
      "        NeuroBasicConditions.__init__(self,self.key,self.dependantVars,self.independantVars) ## FINISH initialization\n",
      "        NeuroTrainer.__init__(self)\n",
      "        \n",
      "\n",
      "\n",
      "        \n",
      "        \n",
      "    def parseconfig(self):\n",
      "        self.neuroStore = str(self.algoConfig[\"algo_id\"]) + \"_Basic.db\n",
      "        self.dependantVars = sorted(self.algoConfig[\"dependantVars\"], key=str.lower)\n",
      "        #independant variables always sorted alphabetically to ensure ID replication\n",
      "        self.independantVars = sorted(self.algoConfig[\"independantVars\"], key=str.lower)\n",
      "        return self\n",
      "        \n",
      "    def build_neural_net(self):\n",
      "        self.mynet = nn.searchnet('_1nn_data.db')\n",
      "        self.mynet.maketables()\n",
      "        ####\n",
      "        '''\n",
      "        self.mynet.generatehiddennode( [   all condition ids for each analyzable dataset   ] , [  all point of interest ids   ] )\n",
      "        \n",
      "        self.mynet.generatehiddennode([dataids['dependant_data'],dataids['independant_data']],allconditionids(phasedeltas))\n",
      "        '''\n",
      "        return self\n",
      "           \n",
      "    def train_neural_net(self):\n",
      "        #aquire data\n",
      "        initDO = SecuFrame(self.key)\n",
      "        timeObjs = initDO.iffo().orderedmap().processFrameList\n",
      "        \n",
      "        for variable in NDC.variable_lst:\n",
      "      \n",
      "            normConst = determineNormConstant(variable) \n",
      "            ND = DataNormalization(timeObjs,variable,normConst)\n",
      "            '''\n",
      "            ND.normalized_dtokey_dict = {}\n",
      "            ND.normalized = []\n",
      "            ND.normalized_dto = []\n",
      "            \n",
      "            ND.normalizedX = []\n",
      "            ND.normalizedX_dto = []\n",
      "            ND.normalizedY = []\n",
      "            '''\n",
      "            \n",
      "            \n",
      "            ## pass data to trainer\n",
      "            #---- trainer to and from conditions\n",
      "            \n",
      "            \n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SecNeural.ClassAlignmentSubsystem import DataNormalization\n",
      "import datetime\n",
      "from SecApp.DBOutputClass import SecuFrame\n",
      "\n",
      "\n",
      "class NeuroTrainer(object):\n",
      "    def __init__(self):\n",
      "        self.initDO = SecuFrame('')\n",
      "        self.fromDB = initDO.iffo().orderedmap().processFrameList\n",
      "        \n",
      "    def methodBasicTrain(self):\n",
      "        for selectionPointVariable in NeuroBasicConditions.pointselectionConfig.keys(): # iterate through point sets\n",
      "            \n",
      "            #Normalized points dataset(object)\n",
      "            normalizedSelectionPointData = DataNormalization(self.fromDB,selectionPointVariable,NeuroBasicConditions.pointselectionConfig[selectionPointVariable][\"NormConstant\"])\n",
      "            normalizedSelectionPointDataID = NeuroBasicConditions.pointselectionConfig[selectionPointVariable][\"ID\"]\n",
      "            #computed selection points\n",
      "            \n",
      "            computePoints = NeuroPoints.__init__(self,normalizedSelectionPointDataID,normalizedSelectionPointData.normalizedX,normalizedSelectionPointData.normalizedY)\n",
      "            computedSelectionPoints = computePoints.XpointsofInterest\n",
      "            for analyzableDatasetVariable in NeuroBasicConditions.datasetConfig.keys(): # iterate through datasets\n",
      "                \n",
      "                #Normalize dataset (object)\n",
      "                normalizedAnalyzableDataset = DataNormalization(self.fromDB,analyzableDatasetVariable,NeuroBasicConditions.datasetConfig[analyzableDatasetVariable][\"NormConstant\"])\n",
      "                normalizedAnalyzableDatasetID = NeuroBasicConditions.datasetConfig[analyzableDatasetVariable]\n",
      "                #compute each condition and train neural network\n",
      "                for computeConditionID in NeuroBasicConditions.DatasetConditions[normalizedAnalyzableDatasetID]: # returns ids of conditions attached to dataset\n",
      "                    \n",
      "                    #initialize conditional search\n",
      "                    ''' finish\n",
      "                    '''\n",
      "                    NeuroCondSearch.__init__(self,datasetID,datasetX,datasetY)\n",
      "                    \n",
      "                    \n",
      "                    for xpoint in computedSelectionPoints: # iterate through each point\n",
      "                        \n",
      "                        #Train neuralNet\n",
      "                    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuroCondSearch(object):\n",
      "    ''' \n",
      "    this class defines all the conditional operations available to be performed on normalized data\n",
      "    \n",
      "    '''\n",
      "\n",
      "    def __init__(self,datasetID,datasetX,datasetY):\n",
      "        \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        #temporary lookup operation\n",
      "        conditionName = NeuroBasicConditions.idLookupCondtion[datasetID]\n",
      "        #assign constants\n",
      "        self.CursorShift = NeuroBasicConditions.conditionsConfig[conditionName]['shift']\n",
      "        self.CursorDelta = NeuroBasicConditions.conditionsConfig[conditionName]['delta']\n",
      "        #assign method\n",
      "        self.method = NeuroBasicConditions.conditionsConfig[conditionName]['condition']\n",
      "                \n",
      "    def runop(self,XSelectionPoint):\n",
      "        \n",
      "        self.XSelectionPoint = XSelectionPoint\n",
      "        \n",
      "        if (self.method == \"increase\"):\n",
      "            return self.Method_increase()\n",
      "    \n",
      "\n",
      "\n",
      "    '''\n",
      "    Methods for processing data, return is always boolean\n",
      "    \n",
      "    '''\n",
      "        \n",
      "    def Method_increase(self):\n",
      "        \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        \n",
      "        #Phase point array\n",
      "        self.XSelectionPoint\n",
      "            \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    '''\n",
      "    processing helpers\n",
      "    '''\n",
      "        \n",
      "    def phase_shift_delta_sample_cursor(self,phase,shift,delta,sample):\n",
      "        '''\n",
      "        self.CursorX = :\n",
      "        self.CursorY =\n",
      "        array[(x_dto,y)], array[(x_dto,y]\n",
      "        \n",
      "        '''\n",
      "        \n",
      "        \n",
      "    \n",
      "    '''\n",
      "    Micro operations\n",
      "    \n",
      "    '''\n",
      "    def sectionincrease(self):\n",
      "        \n",
      "        '''\n",
      "        input section list[(tuple),(tuple)] \n",
      "        (tuple) = \n",
      "        \n",
      "        output tuple(x_dto,boolean)\n",
      "        \n",
      "        self.Cursor\n",
      "        \n",
      "        '''\n",
      "        \n",
      "        #using average of list\n",
      "        av1 = reduce(lambda x, y: x + y, data[0]) / len(data[0])\n",
      "        av2 = reduce(lambda x, y: x + y, data[1]) / len(data[1])\n",
      "        if (av1<av2):\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "        \n",
      "        \n",
      "    def allincreases(self):\n",
      "        '''\n",
      "        \n",
      "        input:\n",
      "        \n",
      "        \n",
      "        '''\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuroPoints(object):\n",
      "    def __init__(self,ID,datasetX,datasetY):      \n",
      "        self.datasetX = datasetX\n",
      "        self.datasetY = datasetY\n",
      "        self.datasetID = ID\n",
      "        \n",
      "        ## must be initialized before hand\n",
      "        self.PointTypeName = NeuroBasicConditions.idLoockupPointSelection[self.datasetID]\n",
      "        self.datasetParams = NeuroBasicConditions.pointselectionConfig[self.PointTypeName]\n",
      "        \n",
      "        ##class variable\n",
      "        NeuroPoints.XpointsofInterest = None\n",
      "        \n",
      "        self.runop()\n",
      "        \n",
      "        \n",
      "    def runop(self):\n",
      "        '''\n",
      "        These must correspond to the methods available in NeuroBasic Conditions under key\n",
      "        self.pointofinteresttype = [\"localmax\"]\n",
      "        '''\n",
      "        if (self.datasetParams[\"selection_algo\"] == \"localmax\" ):\n",
      "            NeuroPoints.XpointsofInterest = self.FindMaximalpos(self.datasetX,self.datasetY)\n",
      "            return True\n",
      "        \n",
      "        \n",
      "     # finds points of interest in dependant variable (phase points)\n",
      "    def FindMaximalpos(self,aX,aY):\n",
      "        '''\n",
      "        Input aligned:\n",
      "        x array\n",
      "        y array\n",
      "        \n",
      "        Output (X):\n",
      "        array[x_dto ,....]\n",
      "        \n",
      "        '''\n",
      "        maximaposX = []\n",
      "        length = len(aY)\n",
      "        if length >= 2:\n",
      "            if aY[0] > aY[1]:\n",
      "                maximaposX.append(0)\n",
      "    \n",
      "           \n",
      "        if length > 3:\n",
      "            for i in range(1, length-1):     \n",
      "                if aY[i] > aY[i-1] and aY[i] > aY[i+1]:\n",
      "                    maximaposX.append(i)\n",
      "    \n",
      "    \n",
      "        if aY[length-1] > aY[length-2]:\n",
      "            maximaposX.append(length-1)\n",
      "           \n",
      "        return maximapos\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuroBasicConditions(object):\n",
      "    def __init__(self,key,depVarsLst,indepVarsLst):\n",
      "        self.depVarsLst = depVarsLst\n",
      "        self.indepVarsLst = indepVarsLst\n",
      "        self.key = key\n",
      "        initQ = SecuQ(self.key)\n",
      "        '''\n",
      "        available parameters used to construct conditions\n",
      "        '''\n",
      "        #analytics conditions\n",
      "        self.baseconditions = [\"increase\"]\n",
      "        self.pointofinteresttype = [\"localmax\"]\n",
      "        #more to be added\n",
      "        #baseconditions = {\"increase\":1,\"decrease\":2,\"MA14_increase\":3,\"MA14_decrease\":4}\n",
      "        self.phasedeltas = [2,3,4,5,6,7,14,21] #how much data in each segment \n",
      "        self.phaseshifts = [1,2,3,4,5,6,7,14,21] # how much data is shifted from phase ( x coord[date])\n",
      "    \n",
      "        '''\n",
      "        constructed conditions (with different maps for easy access)\n",
      "        '''\n",
      "\n",
      "    \n",
      "        NeuroBasicConditions.conditionsConfig = {} #analysis algo conditions\n",
      "        NeuroBasicConditions.conditionLookupId = {}\n",
      "        NeuroBasicConditions.idLookupCondtion = {}\n",
      "        NeuroBasicConditions.allConditionIds = []\n",
      "        NeuroBasicConditions.DatasetConditions = {}\n",
      "        \n",
      "    \n",
      "    \n",
      "        NeuroBasicConditions.pointSelectionConfig = {} #dependant IDs point selection algo\n",
      "        NeuroBasicConditions.pointSelectionLookupId = {}\n",
      "        NeuroBasicConditions.idLookupPointSelection = {}\n",
      "        NeuroBasicConditionsself.allPointSelectionIds = []\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "        NeuroBasicConditions.datasetConfig = {} #independantID datasets available\n",
      "        NeuroBasicConditions.datasetLookupId = {}\n",
      "        NeuroBasicConditions.idLookupDataset = {}\n",
      "        NeuroBasicConditions.allDatasetIds = []\n",
      "        \n",
      "    \n",
      "        '''\n",
      "        this holds starting id numbers for configurtation data \n",
      "        '''\n",
      "        ###\n",
      "        self.depIDcount = 300\n",
      "        \n",
      "        self.indepIDcount = 500\n",
      "        \n",
      "        self.condIDcount = 700\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "    def getNormConstant(self,varkey):  # for variables only\n",
      "        if (varkey in initQ.multipoint.keys()) & (varkey in initQ.aggregate.keys()):\n",
      "            #self.varTypes_v2t_dict[varkey] = 'mp_aggre' # multipoint with aggregate data in dailykey, \"keyname\"_aggregate:value\n",
      "            \n",
      "            #self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'mp_aggre'\n",
      "            raise Exception(\"NO MULTI-AGREGATE VARIABLES (IN ALPHA)\")\n",
      "            return False #breakloop\n",
      "            \n",
      "        if (varkey in initQ.aggregate.keys()):\n",
      "            return datetime.timedelta(days = 1 )\n",
      "        \n",
      "        if (varkey in initQ.multipoint.keys()):\n",
      "            return datetime.timedelta(minutes = 30 )\n",
      "            \n",
      "        if (varkey in initQ.active.keys()):\n",
      "            return datetime.timedelta(days = 1 )\n",
      "  \n",
      "    \n",
      "    \n",
      "    def buildselectionpointIDs(self): #builddependantIDs\n",
      "        for pointselectionalgo in self.pointofinteresttype:\n",
      "            for var in self.depVarsLst:\n",
      "            NeuroBasicConditions.pointselectionConfig[var] = {}\n",
      "            NeuroBasicConditions.pointselectionConfig[var][\"selection_algo\"] = pointselectionalgo\n",
      "            NeuroBasicConditions.pointselectionConfig[var][\"ID\"] = self.depIDcount\n",
      "            NeuroBasicConditions.pointselectionConfig[var][\"NormConstant\"] = self.getNormConstant(var)\n",
      "              \n",
      "            NeuroBasicConditions.pointSelectionLookupId[var] = self.depIDcount\n",
      "            \n",
      "            NeuroBasicConditions.idLookupPointSelection[self.depIDcount] = var\n",
      "            \n",
      "            NeuroBasicConditions.allPointSelectionIds.append(self.depIDcount)\n",
      "            \n",
      "            self.depIDcount+= 1\n",
      "        return self\n",
      "            \n",
      "                \n",
      "            \n",
      "            \n",
      "    def buildindependantIDs(self): #build analyzable (comparable dataset) ids\n",
      "        for var in self.indepVarsLst:\n",
      "            NeuroBasicConditions.datasetConfig[var] = {}\n",
      "            NeuroBasicConditions.datasetConfig[var][\"ID\"] = self.indepIDcount\n",
      "            NeuroBasicConditions.datasetConfig[var][\"NormConstant\"] = self.getNormConstant(var)\n",
      "            \n",
      "            NeuroBasicConditions.datasetLookupId[var] = self.indepIDcount\n",
      "            \n",
      "            NeuroBasicConditions.idLookupDataset[self.indepIDcount] = var\n",
      "            \n",
      "            NeuroBasicConditions.allDatasetIds.append(self.indepIDcount)\n",
      "            \n",
      "            \n",
      "            self.indepIDcount+=1\n",
      "        return self\n",
      "\n",
      "    \n",
      "    def buildconditions(self): # add [haseshifts]\n",
      "        for var in NeuroBasicConditions.datasetConfig.keys():\n",
      "            currVarID = NeuroBasicConditions.datasetConfig[var][\"ID\"]\n",
      "            NeuroBasicConditions.DatasetConditions[currVarID] = []\n",
      "            \n",
      "            for condition in self.baseconditions:\n",
      "                for delta in self.phasedeltas:\n",
      "                    for shift in self.phaseshifts:\n",
      "                        \n",
      "                        \n",
      "                    \n",
      "                        builtconditiontmp = str(condition) + \"-\" + str(currVarID)+ \"-\" + str(delta)+ \"-\" + str(shift)\n",
      "                    \n",
      "                        \n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp] = {}\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp][\"analyze\"] = currVarID\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['condition'] = condition\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['delta'] = delta\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['shift'] = shift\n",
      "                        NeuroBasicConditions.conditionsConfig[builtconditiontmp]['ID'] = self.condIDcount\n",
      "                        \n",
      "                        \n",
      "                        NeuroBasicConditions.DatasetConditions[currVarID].append(self.condIDcount)\n",
      "                        \n",
      "                        NeuroBasicConditions.idLookupCondtion[self.condIDcount] = builtconditiontmp\n",
      "                    \n",
      "                        NeuroBasicConditions.conditionLookupId[builtconditiontmp] = self.condIDcount\n",
      "                    \n",
      "                        NeuroBasicConditions.allConditionIds.append(self.condIDcount)\n",
      "                    \n",
      "                        self.condIDcount+=1\n",
      "        return self\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "from SecApp.DBQuestionClass import SecuQ\n",
      "class NeuroDataComparative:\n",
      "    '''\n",
      "    REQUIRES NoSQL DATABASE ACCESS KEY\n",
      "    Establishes IDs for all data needing to be examined. \n",
      "    \n",
      "    For use in comparing many variables against one dependant\n",
      "    \n",
      "    conditions:\n",
      "    only one dependant variable\n",
      "    \n",
      "    as many independants as you'd like (must be list)\n",
      "    '''\n",
      "    \n",
      "    def __init__(self,key,depVar,indepVarsLst):\n",
      "        self.key = key\n",
      "        initQ = SecuQ(self.key)\n",
      "        \n",
      "        self.variable_v2id_dict = {}\n",
      "        self.variable_id2v_dict = {}\n",
      "        self.variable_lst = []\n",
      "        self.variableids_lst = []\n",
      "        \n",
      "        \n",
      "        self.independantVars = sorted(indepVarsLst, key=str.lower)\n",
      "        self.dependantVar = depVar\n",
      "        self.varTypes_v2t_dict = {}\n",
      "        self.varTypes_id2t_dict = {}\n",
      "        # data IDs start at 900, 900 is reserved for dependant variable, others are after that\n",
      "        \n",
      "        \n",
      "        #class main loop\n",
      "        self.iterateVariables()\n",
      "        \n",
      "        \n",
      "        \n",
      "    def genVariableids(self,varkey):\n",
      "            \n",
      "        self.variable_v2id_dict[varkey] = idcounter\n",
      "        self.variable_id2v_dict[idcounter] = varkey\n",
      "        self.variable_lst.append(varkey)\n",
      "        self.variableids_lst.append(idcounter)\n",
      "        self.idcounter +=1\n",
      "        return self\n",
      "    \n",
      "    #This determines the timedelta or normalization technique\n",
      "    def getanalyticstype(self,varkey): \n",
      "        if (varkey in initQ.multipoint.keys()) & (varkey in initQ.aggregate.keys()):\n",
      "            #self.varTypes_v2t_dict[varkey] = 'mp_aggre' # multipoint with aggregate data in dailykey, \"keyname\"_aggregate:value\n",
      "            \n",
      "            #self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'mp_aggre'\n",
      "            raise Exception(\"NO MULTI-AGREGATE VARIABLES (IN ALPHA)\")\n",
      "            return False #breakloop\n",
      "            \n",
      "        if (varkey in initQ.aggregate.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'aggregate' # aggregate data in dailykey\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'aggregate'\n",
      "            return self #breakloop \n",
      "        \n",
      "        if (varkey in initQ.multipoint.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'multipoint'\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'multipoint'\n",
      "            return self #breakloop\n",
      "            \n",
      "        if (varkey in initQ.active.keys()):\n",
      "            self.varTypes_v2t_dict[varkey] = 'daily' # data in dailykey\n",
      "            self.varTypes_id2t_dict[self.variable_v2id_dict[varkey]] = 'daily'\n",
      "            return self #breakloop\n",
      "\n",
      "        \n",
      "    def iterateVariables(self):\n",
      "        #dependant first\n",
      "        self.idcounter = 900\n",
      "        self.genVariableids(self.dependantVar)\n",
      "        self.getanalyticstype(self.dependantVar)\n",
      "        \n",
      "        #independants in alphabetical order\n",
      "        for var in self.independantVars:\n",
      "            self.genVariableids(var)\n",
      "            self.getanalyticstype(var)\n",
      "            \n",
      "        return self\n",
      "        \n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}